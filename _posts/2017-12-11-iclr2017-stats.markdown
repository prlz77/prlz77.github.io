---
layout: post
title: Regularizing CNNs with Locally Constrained Decorrelations (ICLR2017)
date: 2017-03-12 22:00
description: Regularizing CNNs with Locally Constrained Decorrelations
comments: true
categories:
- blog
permalink: iclr2017-paper/
---

**TL;DR** We propose to locally decorrelate the feature weights of CNNs. When
the proposed method, which we call
[OrthoReg](https://openreview.net/pdf?id=ByOvsIqeg), is used to regularize the 40
layers of Wide Residual Networks, we obtain state of the art results on CIFAR,
and SVHN. Here is an example of the effects of our local regularizer:

<script type="text/javascript" src="{{ site.url }}/assets/js/d3.v4.min.js"></script>
<script type="text/javascript" src="{{ site.url }}/assets/js/numeric-1.2.6.min.js"></script>
<link rel="stylesheet" type="text/css" href="{{ site.url }}/assets/css/iclr2017/main.css">
<div class="controls">
<div class="control-container" >
<b title="Number of data points.">Dataset Size</b><br>
<input id="N" type="text" name="size" value="10" maxlength="4" onkeypress="if
(event.keyCode == 13) {scatterStart(); }">
</div>
<div class="control-container" >
<b title="Size of the steps that the dots make to separate. A large number might cause divergence, and a small number will make the algorithm too slow.">Step Size</b><br>
<input id="alpha" type="text" name="alpha" value="0.1" onkeypress="if
(event.keyCode == 13) {scatterStart(); }">
</div>
<div class="control-container" >
<b title="The radius of influence of each point. The greater the lambda, the smaller the radius">Lambda</b><br>
<input id="lambda" type="text" name="lambda" value="10" maxlength="4"
onkeypress="if (event.keyCode == 13) {scatterStart(); }">
</div>
<div class="control-container" >
<b title="Max number of iterations until stop.">Max Iter</b><br>
<input id="maxIter" type="text" name="maxIter" value="20" onkeypress="if
(event.keyCode == 13) {scatterStart(); }">
</div>
<div class="control-container" >
<b title="Stop the algorithm when the dots move less than a delta (plateau).">Delta Stop</b><br>
<input id="delta" type="text" name="delta" value="0.001" onkeypress="if
(event.keyCode == 13) {scatterStart(); }">
</div>
<div class="control-container" >
<input type="button" onclick="scatterStart();" value="play">
</div>
</div>
<div class="graph-container">
<div id="baseline" class="three-column" style="margin-left:10%; border-right: 1px
solid lightgray;"><b style="color: steelblue;">Baseline</b></div>
<div id="orthoreg" class="three-column" style="margin-right:10%;"><b
style="color: red;">Ours</b></div>
<div id="angle" class="one-column" style="text-align:left;"> 
<b >Average nearest neighbor distance</b></div>
</div>
<script type="text/javascript" src="{{ site.url
}}/assets/js/iclr2017/d3-plots-min.js"></script>
<script> document.onload = scatterStart(); </script>

I encourage the reader to continue reading for better understanding of this
example.
<!---->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

Deep learning models are becoming increasingly bigger and deeper, beating the
state of the art in numerous tasks. But how is it possible to grow these neural
net architectures while preventing them from completely memorizing the training
data and showing good performances on unseen examples (overfitting)? The answer to
this question includes multiple factors such as having bigger datasets, clever
architectures, and good regularization methods. Our work is centered on the
latter. Concretely, we focus on those regularization methods that prevent the
co-adaptation of feature detectors, thus reducing the redundancy in the models
and increasing their generalization power.

Probably, the most popular way to prevent feature co-adaptation is
[Dropout](https://github.com/szagoruyko/wide-residual-networks). However,
dropping techniques such as Dropout or
[DropConnect](http://cs.nyu.edu/~wanli/dropc/) reduce the capacity of the
models, which need to be bigger to compensate for this fact.
A least common method for preventing co-adaptation is to
add a regularization term to the loss function of the model so as to penalize
the correlation between neurons. This idea has been explored several times for
supervised learning on neural networks,
first with the name of [incoherent
training](http://ieeexplore.ieee.org/document/6639015/), and later as [DeCov](https://arxiv.org/abs/1511.06068)
(Cogswell et al. at ICLR2016).

Although the presented decorrelation methods proved to perform well, they are
still far from being commonly used in state of the art models. In part, this is
because they were applied to rather shallow models, and it is not
clear that the computational overhead introduced by these regularizers in state
of the art models will compensate the reduction in overfitting. Our work aims to
dissipate these inconveniences by:

* reducing the computational cost by regularizing the network weights,
* increasing the performance margins by imposing locality constraints,
* successfully regularizing all the layers of a state of the art residual network.

Why imposing locality when decorrelating feature detectors? Since a toy is more
amusing than one thousand words, please <a href="#baseline">play</a> with the example I provide at the
beginning of the post (if you have not already tried it ;) ). Note that although
it is a toy example with 2D nice-to-plot features, similar behavior was found in
actual CNN features, especially in bottleneck layers, when the number of filters
matches their dimensionality. 
The intuition is
that regularizing negatively decorrelated feature detectors is
counter-productive. Namely, in the <a href="#baseline">left plot</a>, each dot
(feature detector) "tries to be different" from all the other dots, even those which are in the
opposite quadrant. We propose to make dots only sensitive to their nearest
neighbors, thus increasing the average minimum angle (linear correlation) between all the feature
detectors.  We achieve this locality by means of a squashing function:

$$
    C(\theta) = \sum_{i=1}^{n}\sum_{j=1,j\ne i}^{n} \log(1 + e^{\lambda
    (cos(\theta_i,\theta_j)-1)}) = \log(1 + e^{\lambda  (\langle
    \theta_i,\theta_j\rangle -1)}), \ ||\theta_i||=||\theta_j||=1,
$$

where $\lambda$ is a coefficient that controls the minimum
angle-of-influence of the regularizer, i.e. the minimum angle between two
feature weights so that there exists a gradient pushing them apart. Again, a toy
is worth a thousand words:

<script
src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<link rel="stylesheet"
href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/themes/smoothness/jquery-ui.css">
<script
src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js"></script>
<div class="graph-container">
<div id="squashing" style="float:left; width:55%;
margin-left:23%;" ></div><div style="position:absolute; left:75%; margin-top:25%;"><span
style="color:steelblue">$cos^2(x)$</span><br><span style="color:red;">$\log(1 +
e^{\lambda  (x-1)})$</span></div>
<div style="width:40%; margin:15px; margin-left:23%; float:left;"
id="slider"></div><div
style="float:left; margin:15px;" type="text">$\lambda=$<span
id="slider-value">10</span></div>
</div>
<script>
var squashingGraph = new SquashingGraph();
$( function() {
    $( "#slider" ).slider({
        min: 2,
        max: 20,
        step: 1,
        value: 10,
        animate: "fast",
        slide: function (event, ui) { squashingGraph.updateGraph(ui.value);
        $("#slider-value").text(ui.value); }
    });
} );
</script>

As it can be seen, when $\lambda \in \[7,10\]$, the squashing function gradient
approximates zero for those features further than $\pi / 2$ (90º). In other
words, the regularizer will enforce feature weights to be orthogonal. As a
result, the linear correlation between feature detectors will decrease,
resulting in better generalization as it can be seen in the following figure:

<div class="graph-container"><div style="width:50%; float:left;">
<img src="{{ site.url }}/assets/images/iclr2017/Cifar10.svg" />
</div>
<div style="width:50%; float:left;">
<img src="{{ site.url }}/assets/images/iclr2017/Cifar100.svg" />
</div>
</div>

Note that OrthoReg is able to reduce the overfitting in the presence of Dropout
and [BatchNorm](https://arxiv.org/abs/1502.03167) at the same time. A possible
hypothesis for this fact is that different regularizers act on different
aspects of the network, exposing how much is still to be explored if we want to
comprehend how to reach Deep Learning's full potential.

I am delighted that you, the reader, have arrived at this line, for it means that I
have been able to capture your attention and you have decided to
spend some moments of your precious time to give a meaning to this work by
reading it (and I hope you played a little bit too). 

## Acknowledgements
Thanks to [@pepgonfaus](https://twitter.com/), [@gcucurull](https://github.com/gcucurull),
and Jordi Gonzàlez for their comments and suggestions. 
